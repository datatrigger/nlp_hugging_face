{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP with ðŸ¤— Hugging Face\n",
    "\n",
    "## Text Classification: Transfer Learning vs Zero-Shot Classifier\n",
    "\n",
    "ðŸ¤— [Hugging Face](https://huggingface.co/) is, in my opinion, one of the best things that has happened to Data Science over the past few years. From generalizing access to state-of-the-art NLP models with the [`transformers`](https://huggingface.co/transformers/) library to [distillation [1]](https://arxiv.org/abs/1910.01108), they are having a huge impact on the field. I recently found out about \"Zero-Shot Classification\". These models are classifiers that do not need any fine-tuning, apart from being told which classes it should predict. They are built on top of Natural Language Inference models, whose task is determining if sentence *A* implies, contradicts or has nothing to do with sentence *B*. This excellent [blog post](https://joeddav.github.io/blog/2020/05/29/ZSL.html) written by ðŸ¤— Hugging Face researcher Joe Davison provides more in-depth explanations.  \n",
    "  \n",
    "Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas\n",
    "# %pip install torch\n",
    "# %pip install scipy\n",
    "# %pip install transformers\n",
    "# %pip install sklearn\n",
    "# %pip install matplotlib\n",
    "# %pip install seaborn\n",
    "# %pip install numpy\n",
    "# %pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli (https://huggingface.co/facebook/bart-large-mnli)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'Parallel processing with GPUs is the savior of Deep Learning',\n",
       " 'labels': ['technology', 'education', 'politics'],\n",
       " 'scores': [0.9941806197166443, 0.0031009134836494923, 0.0027184567879885435]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transformers 4.20.1 in this notebook\n",
    "from transformers import pipeline\n",
    "\n",
    "# By default, the pipeline runs on the CPU (device=-1). Set device to 0 to use the GPU (and to 1 for the second GPU, etc...)\n",
    "classifier = pipeline(\"zero-shot-classification\", device=0)\n",
    "classifier(\n",
    "    \"Parallel processing with GPUs is the savior of Deep Learning\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"technology\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier guessed that the sentence is about tech with a probability over 99%. **But how does Zero-Shot classification compare with plain \"old\" fine-tuned text classification?**\n",
    "\n",
    "### I) BBC News dataset\n",
    "\n",
    "Let's build a classifier of news articles labeled *business*, *entertainment*, *politics*, *sport* and *tech*. Available [here](http://mlg.ucd.ie/datasets/bbc.html), the dataset consists of 2225 documents from the BBC news website from the years 2004/2005. It was originally built for a Machine Learning paper about clustering [[2]](http://mlg.ucd.ie/files/publications/greene06icml.pdf).  \n",
    "  \n",
    "Articles are individual .txt files spread into 5 folders, one for each folder. The listing below puts articles/labels into a `pandas.DataFrame()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities to handle directories, files, paths, etc...\n",
    "from os import listdir\n",
    "from os.path import isdir, isfile, join\n",
    "from pathlib import Path\n",
    "\n",
    "# Most original import ever\n",
    "import pandas as pd\n",
    "\n",
    "path_to_bbc_articles=\"bbc\"\n",
    "labels=[] # labels for the text classification\n",
    "label_dataframes=[] # for each label, get the articles into a dataframe\n",
    "\n",
    "for label in [dir for dir in listdir(path_to_bbc_articles) if isdir(join(path_to_bbc_articles, dir)) and dir!=\".ipynb_checkpoints\"]:\n",
    "    labels.append(label)\n",
    "    label_path=join(path_to_bbc_articles, label)\n",
    "    articles_list=[]\n",
    "    for article_file in [file for file in listdir(label_path) if isfile(join(label_path, file))]:\n",
    "        article_path=join(label_path, article_file)\n",
    "        article=Path(article_path).read_text(encoding=\"ISO-8859-1\") # Tried utf-8 (of course) but encountered error\n",
    "        # Stackoverflow said \"try ISO-8859-1\", it worked (dataset is 11 years old)\n",
    "        articles_list.append(article)\n",
    "    label_dataframes.append(pd.DataFrame({'label': label, 'article': articles_list}))\n",
    "    \n",
    "df=pd.concat(label_dataframes, ignore_index=True) # Concatenate all the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "sport            511\n",
       "business         510\n",
       "politics         417\n",
       "tech             401\n",
       "entertainment    386\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of articles per label\n",
    "df.value_counts('label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need integer labels to feed the transformer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label_int']=df['label'].apply(lambda x:labels.index(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are 5 random rows from the final dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>article</th>\n",
       "      <th>label_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>business</td>\n",
       "      <td>UK house prices dip in November\\n\\nUK house pr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>business</td>\n",
       "      <td>LSE 'sets date for takeover deal'\\n\\nThe Londo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1644</th>\n",
       "      <td>sport</td>\n",
       "      <td>Harinordoquy suffers France axe\\n\\nNumber eigh...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>business</td>\n",
       "      <td>Barclays shares up on merger talk\\n\\nShares in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232</th>\n",
       "      <td>politics</td>\n",
       "      <td>Campaign 'cold calls' questioned\\n\\nLabour and...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                            article  label_int\n",
       "414   business  UK house prices dip in November\\n\\nUK house pr...          0\n",
       "420   business  LSE 'sets date for takeover deal'\\n\\nThe Londo...          0\n",
       "1644     sport  Harinordoquy suffers France axe\\n\\nNumber eigh...          3\n",
       "416   business  Barclays shares up on merger talk\\n\\nShares in...          0\n",
       "1232  politics  Campaign 'cold calls' questioned\\n\\nLabour and...          2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II) Fine-tuning a pretrained text classifier\n",
    "\n",
    "After building the train/validation/test sets, we will go straight the point by using the [`DistilBERT`](https://huggingface.co/transformers/model_doc/distilbert.html) pre-trained transformer model (and its tokenizer).\n",
    "\n",
    "> *It is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERTâ€™s performances.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set, validation set and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val, test = train_test_split(df, test_size=0.1, random_state=42, shuffle=True)\n",
    "train, val = train_test_split(train_val, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Reset the indexes of the 3 pandas.DataFrame()\n",
    "train, val, test = map(lambda x:x.reset_index(drop=True), [train, val, test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize\n",
    "\n",
    "Loading DistilBERT's tokenizer, we can see that this transformer model takes input sequences composed of up to 512 tokens: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'distilbert-base-uncased': 512,\n",
       " 'distilbert-base-uncased-distilled-squad': 512,\n",
       " 'distilbert-base-cased': 512,\n",
       " 'distilbert-base-cased-distilled-squad': 512,\n",
       " 'distilbert-base-german-cased': 512,\n",
       " 'distilbert-base-multilingual-cased': 512}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Distilbert's tokenizer\n",
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "tokenizer.max_model_input_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this compare with the lengths of the tokenized BBC articles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (573 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2225.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>487.627416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>293.625901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>108.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>314.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>423.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>592.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5303.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            length\n",
       "count  2225.000000\n",
       "mean    487.627416\n",
       "std     293.625901\n",
       "min     108.000000\n",
       "25%     314.000000\n",
       "50%     423.000000\n",
       "75%     592.000000\n",
       "max    5303.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_articles_lengths=pd.DataFrame({'length': list(map(len, tokenizer(df['article'].to_list(), truncation=False, padding=False)['input_ids']))})\n",
    "tokenized_articles_lengths.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The articles are, on average, 488-token-long. The longest news is composed of 5303 tokens. This means that an important part of the articles will be truncated before being fed to the transformer model. Here is the distribution of the lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "ax=sns.histplot(tokenized_articles_lengths)\n",
    "ax.set(xlabel='Length of tokenized articles', ylabel='Count', xlim=(0, 1200), title='Distribution of the tokenized articles lengths')\n",
    "plt.show()\n",
    "\n",
    "# As shown on datatrigger.org/nlp_hugging_face:\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(16, 16))\n",
    "# ax=sns.histplot(tokenized_articles_lengths, palette='dark')\n",
    "# ax.set(xlim=(0, 1200))\n",
    "#Â ax.set_xticks(range(0, 1200, 100))\n",
    "#Â ax.set_title('Distribution of the tokenized articles lengths', fontsize=24, pad=20)\n",
    "# ax.set_xlabel('Length of tokenized articles', fontsize = 18, labelpad = 10)\n",
    "# ax.set_ylabel('Count', fontsize = 18, labelpad = 10)\n",
    "# ax.tick_params(labelsize=14)\n",
    "# plt.savefig('tokenized_articles_length_distribution.png', bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentile of length=512: 64th\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import percentileofscore\n",
    "print(f'Percentile of length=512: {int(percentileofscore(tokenized_articles_lengths[\"length\"],512))}th')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 36% of the articles will be truncated to fit the 512-token limit of DistilBERT. The truncation is mandatory, otherwise the model crashes. We will use fixed padding for the sake of simplicity here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tune DistilBERT\n",
    "\n",
    "The train/validation/test sets must be procesÎ¼sed to work with either PyTorch or TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the train/validation/test sets\n",
    "train_encodings = tokenizer(train['article'].to_list(), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val['article'].to_list(), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test['article'].to_list(), truncation=True, padding=True)\n",
    "\n",
    "import torch\n",
    "\n",
    "class BBC_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = BBC_Dataset(train_encodings, train['label_int'].to_list())\n",
    "val_dataset = BBC_Dataset(val_encodings, val['label_int'].to_list())\n",
    "test_dataset = BBC_Dataset(test_encodings, test['label_int'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning\n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# The number of predicted labels must be specified with num_labels\n",
    "# .to('cuda') to do the training on the GPU\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(labels)).to('cuda')\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1601\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 603\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='603' max='603' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [603/603 04:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.140300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n",
      "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=603, training_loss=0.118651934919468, metrics={'train_runtime': 242.7883, 'train_samples_per_second': 19.783, 'train_steps_per_second': 2.484, 'total_flos': 636274955105280.0, 'train_loss': 0.118651934919468, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bbc_news_model\n",
      "Configuration saved in bbc_news_model/config.json\n",
      "Model weights saved in bbc_news_model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"bbc_news_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 223\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56' max='56' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [56/56 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate predictions for the test set\n",
    "predictions=trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test_results=test.copy(deep=True)\n",
    "test_results[\"label_int_pred_transfer_learning\"]=np.argmax(predictions.predictions, axis=-1)\n",
    "test_results['label_pred_transfer_learning']=test_results['label_int_pred_transfer_learning'].apply(lambda x:labels[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the fine-tuned DistilBERT transformer model on the test set is 98.65%.\n"
     ]
    }
   ],
   "source": [
    "accuracy=1-(len(test_results[test_results[\"label\"]!=test_results[\"label_pred_transfer_learning\"]])/len(test_results))\n",
    "print(f'The accuracy of the fine-tuned DistilBERT transformer model on the test set is {100*accuracy:.2f}%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Misclassified articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>article</th>\n",
       "      <th>label_int</th>\n",
       "      <th>label_int_pred_transfer_learning</th>\n",
       "      <th>label_pred_transfer_learning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>business</td>\n",
       "      <td>G7 backs Africa debt relief plan\\n\\nG7 finance ministers have backed plans to write off up to 100% of the debts of some of the world's poorest countries.\\n\\nUK chancellor Gordon Brown said the London meeting of the world's seven richest nations would be remembered as \"the 100% debt relief summit\". Some 37 countries could benefit after a case-by-case review by bodies including the World Bank and the IMF, he said. But the US says it cannot support Mr Brown's International Finance Facility to boost aid to developing countries. BBC correspondents said the meeting had produced some movement towards the UK's ambitions, but much work was needed. Mr Brown said it was a major breakthrough for the international organisations to offer up to 100% multilateral debt relief - \"the vast bulk\" of money owed by the poorest countries.\\n\\n\"We could be at the beginning of the final stage of the process where the debts that were owed by the poorest countries, built up over 20 or 30 years, debts that are simply unpayable in the real world, are finally taken care of,\" he said. He added: \"It is the richest countries hearing the voices of the poor.\" But he said they would insist on government reforms and the need for transparency, tackling corruption and openness from both the poorest and richest nations. BBC correspondent Patrick Bartlett said while it was an agreement in principle, the organisations involved now have to look at how it would work in practice. Oxfam senior policy adviser Max Lawson welcomed the statement and said G7 ministers had \"passed the first hurdle of 2005\".\\n\\nBut he added: \"They need to move quickly to turn their proposals into real change for the world's poorest. \"Two million children will die needlessly between now and the next meeting in April. If rich countries are going to keep their promises to tackle obscene poverty they need deliver - and deliver quickly.\" Talks are continuing on how to finance increased overseas development assistance. The International Monetary Fund (IMF) is to look at a proposal to use its gold supplies to help the debt relief effort when it meets in April. Mr Brown said G7 ministers had agreed to defer debt interest payments and repayments for some countries affected by the tsunami until the end of 2005. But UK plans for an International Finance Facility (IFF) to help deal with debt in the developing world have not been agreed. Mr Brown wanted to provide $10bn (Ã‚Â£5.38bn) a year over a decade, using G7 backing so the money could be borrowed up front on financial markets.\\n\\nIt is a key element of his proposals for a modern version of the Marshall Plan, which brought US aid to rebuild Europe after World War II, for the developing world. Mr Brown said it was \"winning support every day\" and said a programme had been agreed to draw up more details in time for the G8 summit in July. But US Treasury Under-Secretary John Taylor said the US could not support the IFF because of its \"legislative process\". \"The US is completely committed to poverty reduction and providing financing to do that,\" he said. \"But this particular mechanism does not work for the United States. It works for other countries, and that is fine.\" Earlier, he told BBC Radio 4's Today programme the US had increased support for Africa in the past four years from $1.1bn per year to $4.6bn per year. But South Africa Finance Minister Trevor Manuel told the BBC's Talking Point programme what was needed was one approach, with all wealthy nations on board. He said much of the money pledged by the US had not yet been dispensed.\\n\\nThe UK has made poverty in the poorest nations a key theme for its 2005 presidency of the Group of Eight (G8), which comprises the G7 and Russia. The G8 countries will meet at Gleneagles in Scotland. At a dinner on Friday night, former South African president Nelson Mandela backed Mr Brown's plan when he urged the finance chiefs to write-off African debt and provide an extra $50bn (Ã‚Â£26.69bn) a year in aid for the next decade. Talks also centred on the impact of the rising economies of China and India, the US budget and trade deficits, how the US, Europe and Japan can act to boost global economic growth, and HIV/Aids. G7 ministers called for more flexibility in international exchange rates and said \"excess volatility\" would impede economic growth. Representatives from China, India, Russia, South Africa and Brazil were invited to attend some of the sessions. A G8 summit is set to take place in July.\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>Uganda bans Vagina Monologues\\n\\nUganda's authorities have banned the play The Vagina Monologues, due to open in the capital, Kampala this weekend.\\n\\nThe Ugandan Media Council said the performance would not be put on as it promoted and glorified acts such as lesbianism and homosexuality. It said the production could go ahead if the organisers \"expunge all the offending parts\". But the organisers of the play say it raises awareness of sexual abuse against women. \"The play promotes illegal, unnatural sexual acts, homosexuality and prostitution, it should be and is hereby banned,\" the council's ruling said.\\n\\nThe show, which has been a controversial sell-out around the world, explores female sexuality and strength through individual women telling their stories through monologues. Some parliamentarians and church leaders are also siding with the Media Council, Uganda's New Vision newspaper reports. \"The play is obscene and pornographic although it was under the guise of women's liberation,\" MP Kefa Ssempgani told parliament.\\n\\nBut the work's author, US playwright Eve Ensler, says it is all about women's empowerment. \"There is obviously some fear of the vagina and saying the word vagina,\" Ms Ensler told the BBC. \"It's not a slang word or dirty word it's a biological, anatomical word.\" She said the play is being produced and performed by Ugandan women and it is not being forced on them. The four Ugandan NGOs organising the play intended to raise money to campaign to stop violence against women and to raise funds for the war-torn north of the country. \"I'm extremely outraged at the hypocrisy,\" the play's organiser in Uganda, Sarah Mukasa, told the BBC's Focus on Africa programme. \"I'm amazed that this country Uganda gives the impression that it is progressive and supports women's rights and the notions of free speech; yet when women want to share their stories the government uses the apparatus of state to shut us up.\"\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>politics</td>\n",
       "      <td>UK firms 'embracing e-commerce'\\n\\nUK firms are embracing internet trading opportunities as never before, e-commerce minister Mike O'Brien says.\\n\\nA government-commissioned study ranked the UK third in its world index of use of information and communication technology (ICT). The report suggests 69% of UK firms are now using broadband and that 30% of micro businesses are trading online. Mr O'Brien said UK businesses were sprinting forward in ICT use, but that there were more challenges ahead. The report, carried out independently by consultants Booz Allen Hamilton and HI Europe, placed the UK third behind Sweden and Ireland for business use of ICT.\\n\\nIt showed British business brought greater maturity to their ICT use, by using broadband in increased numbers, bringing ICT into their business plans and using new technologies such as voice activated programmes and desktop video conferences. Mr O'Brien said: \"The increase in the proportion of business connected by broadband shows that UK companies are embracing the opportunities that ICT can bring. \"It is particularly encouraging to see that small businesses are beginning to narrow the digital divide that appeared to have opened up in recent years.\" The government would play its part in \"cultivating an environment where information and communication technologies can flourish\", Mr O'Brien said. The \"clear message\" the report sends is that effective use of ICT can bring real improvements in business performance for all business.\\n\\n\"However, we are not at the finishing line yet and many challenges remain if the UK is to reach its aim of becoming a world-leading e-economy,\" he added. The International Benchmarking Study was based on 8,000 telephone interviews with businesses, of which more than 2,700 were UK businesses. It is the eighth in a series of examining the adoption and deployment of ICT in the world's most industrialised nations.\\n</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             label  \\\n",
       "63        business   \n",
       "120  entertainment   \n",
       "211       politics   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     article  \\\n",
       "63   G7 backs Africa debt relief plan\\n\\nG7 finance ministers have backed plans to write off up to 100% of the debts of some of the world's poorest countries.\\n\\nUK chancellor Gordon Brown said the London meeting of the world's seven richest nations would be remembered as \"the 100% debt relief summit\". Some 37 countries could benefit after a case-by-case review by bodies including the World Bank and the IMF, he said. But the US says it cannot support Mr Brown's International Finance Facility to boost aid to developing countries. BBC correspondents said the meeting had produced some movement towards the UK's ambitions, but much work was needed. Mr Brown said it was a major breakthrough for the international organisations to offer up to 100% multilateral debt relief - \"the vast bulk\" of money owed by the poorest countries.\\n\\n\"We could be at the beginning of the final stage of the process where the debts that were owed by the poorest countries, built up over 20 or 30 years, debts that are simply unpayable in the real world, are finally taken care of,\" he said. He added: \"It is the richest countries hearing the voices of the poor.\" But he said they would insist on government reforms and the need for transparency, tackling corruption and openness from both the poorest and richest nations. BBC correspondent Patrick Bartlett said while it was an agreement in principle, the organisations involved now have to look at how it would work in practice. Oxfam senior policy adviser Max Lawson welcomed the statement and said G7 ministers had \"passed the first hurdle of 2005\".\\n\\nBut he added: \"They need to move quickly to turn their proposals into real change for the world's poorest. \"Two million children will die needlessly between now and the next meeting in April. If rich countries are going to keep their promises to tackle obscene poverty they need deliver - and deliver quickly.\" Talks are continuing on how to finance increased overseas development assistance. The International Monetary Fund (IMF) is to look at a proposal to use its gold supplies to help the debt relief effort when it meets in April. Mr Brown said G7 ministers had agreed to defer debt interest payments and repayments for some countries affected by the tsunami until the end of 2005. But UK plans for an International Finance Facility (IFF) to help deal with debt in the developing world have not been agreed. Mr Brown wanted to provide $10bn (Ã‚Â£5.38bn) a year over a decade, using G7 backing so the money could be borrowed up front on financial markets.\\n\\nIt is a key element of his proposals for a modern version of the Marshall Plan, which brought US aid to rebuild Europe after World War II, for the developing world. Mr Brown said it was \"winning support every day\" and said a programme had been agreed to draw up more details in time for the G8 summit in July. But US Treasury Under-Secretary John Taylor said the US could not support the IFF because of its \"legislative process\". \"The US is completely committed to poverty reduction and providing financing to do that,\" he said. \"But this particular mechanism does not work for the United States. It works for other countries, and that is fine.\" Earlier, he told BBC Radio 4's Today programme the US had increased support for Africa in the past four years from $1.1bn per year to $4.6bn per year. But South Africa Finance Minister Trevor Manuel told the BBC's Talking Point programme what was needed was one approach, with all wealthy nations on board. He said much of the money pledged by the US had not yet been dispensed.\\n\\nThe UK has made poverty in the poorest nations a key theme for its 2005 presidency of the Group of Eight (G8), which comprises the G7 and Russia. The G8 countries will meet at Gleneagles in Scotland. At a dinner on Friday night, former South African president Nelson Mandela backed Mr Brown's plan when he urged the finance chiefs to write-off African debt and provide an extra $50bn (Ã‚Â£26.69bn) a year in aid for the next decade. Talks also centred on the impact of the rising economies of China and India, the US budget and trade deficits, how the US, Europe and Japan can act to boost global economic growth, and HIV/Aids. G7 ministers called for more flexibility in international exchange rates and said \"excess volatility\" would impede economic growth. Representatives from China, India, Russia, South Africa and Brazil were invited to attend some of the sessions. A G8 summit is set to take place in July.\\n   \n",
       "120                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Uganda bans Vagina Monologues\\n\\nUganda's authorities have banned the play The Vagina Monologues, due to open in the capital, Kampala this weekend.\\n\\nThe Ugandan Media Council said the performance would not be put on as it promoted and glorified acts such as lesbianism and homosexuality. It said the production could go ahead if the organisers \"expunge all the offending parts\". But the organisers of the play say it raises awareness of sexual abuse against women. \"The play promotes illegal, unnatural sexual acts, homosexuality and prostitution, it should be and is hereby banned,\" the council's ruling said.\\n\\nThe show, which has been a controversial sell-out around the world, explores female sexuality and strength through individual women telling their stories through monologues. Some parliamentarians and church leaders are also siding with the Media Council, Uganda's New Vision newspaper reports. \"The play is obscene and pornographic although it was under the guise of women's liberation,\" MP Kefa Ssempgani told parliament.\\n\\nBut the work's author, US playwright Eve Ensler, says it is all about women's empowerment. \"There is obviously some fear of the vagina and saying the word vagina,\" Ms Ensler told the BBC. \"It's not a slang word or dirty word it's a biological, anatomical word.\" She said the play is being produced and performed by Ugandan women and it is not being forced on them. The four Ugandan NGOs organising the play intended to raise money to campaign to stop violence against women and to raise funds for the war-torn north of the country. \"I'm extremely outraged at the hypocrisy,\" the play's organiser in Uganda, Sarah Mukasa, told the BBC's Focus on Africa programme. \"I'm amazed that this country Uganda gives the impression that it is progressive and supports women's rights and the notions of free speech; yet when women want to share their stories the government uses the apparatus of state to shut us up.\"\\n   \n",
       "211                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           UK firms 'embracing e-commerce'\\n\\nUK firms are embracing internet trading opportunities as never before, e-commerce minister Mike O'Brien says.\\n\\nA government-commissioned study ranked the UK third in its world index of use of information and communication technology (ICT). The report suggests 69% of UK firms are now using broadband and that 30% of micro businesses are trading online. Mr O'Brien said UK businesses were sprinting forward in ICT use, but that there were more challenges ahead. The report, carried out independently by consultants Booz Allen Hamilton and HI Europe, placed the UK third behind Sweden and Ireland for business use of ICT.\\n\\nIt showed British business brought greater maturity to their ICT use, by using broadband in increased numbers, bringing ICT into their business plans and using new technologies such as voice activated programmes and desktop video conferences. Mr O'Brien said: \"The increase in the proportion of business connected by broadband shows that UK companies are embracing the opportunities that ICT can bring. \"It is particularly encouraging to see that small businesses are beginning to narrow the digital divide that appeared to have opened up in recent years.\" The government would play its part in \"cultivating an environment where information and communication technologies can flourish\", Mr O'Brien said. The \"clear message\" the report sends is that effective use of ICT can bring real improvements in business performance for all business.\\n\\n\"However, we are not at the finishing line yet and many challenges remain if the UK is to reach its aim of becoming a world-leading e-economy,\" he added. The International Benchmarking Study was based on 8,000 telephone interviews with businesses, of which more than 2,700 were UK businesses. It is the eighth in a series of examining the adoption and deployment of ICT in the world's most industrialised nations.\\n   \n",
       "\n",
       "     label_int  label_int_pred_transfer_learning label_pred_transfer_learning  \n",
       "63           0                                 2                     politics  \n",
       "120          1                                 2                     politics  \n",
       "211          2                                 4                         tech  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(test_results[test_results[\"label\"]!=test_results[\"label_pred_transfer_learning\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 3 errors on the test set, out of 56 articles. Besides, the misclassifications actually make sense because the articles could very well have been labelled differently:\n",
    "\n",
    "* The first article was put into the category *business* by the labeller whereas the model predicted *politics*. But since it is about the richest countries' ministers/presidents writing off some African countries' debt, it is also definitely about politics\n",
    "* The second article is about the censorship of a play in Uganda, so the predicted label *politics* could actually be deemed more relevant than the original label, *entertainment*\n",
    "* The third misclassified article is about the impact of IT on UK's firms, mainly through *e*-commerce, and the role of the government in this area. So *politics* (actual), *tech* (predicted) and even *business* are fine as labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III) Zero-Shot Classification\n",
    "\n",
    "We'll use the appropriate [`transformers.pipeline`](https://huggingface.co/transformers/main_classes/pipelines.html) to compute the predicted class for each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\", device=0) # device=0 means GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/pipelines/base.py:1012: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Compute the predicted label for each article\n",
    "test_results['label_pred_zero_shot']=test_results['article'].apply(lambda x:classifier(x, candidate_labels=labels)['labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns and save results\n",
    "test_results=test_results[['article', 'label', 'label_pred_transfer_learning', 'label_pred_zero_shot']]\n",
    "test_results.to_parquet(\"test_results.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results=pd.read_parquet(\"test_results.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Zero-Shot classifier: 58.74 %\n"
     ]
    }
   ],
   "source": [
    "error_rate=len(test_results[test_results[\"label\"]!=test_results[\"label_pred_zero_shot\"]])/len(test_results)\n",
    "print(f'Accuracy of the Zero-Shot classifier: {round(100*(1-error_rate), 2)} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Zero-Shot classifier does a really bad job compared with the fine-tuned model. However, given the number of labels &mdash; 5 &mdash; this result is not that catastrophic. It is well above the 20% a random classifier would achieve (assuming balanced classes). Let's have a look at a few random articles uncorrectly labeled by the Zero-Shot classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>label</th>\n",
       "      <th>label_pred_transfer_learning</th>\n",
       "      <th>label_pred_zero_shot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Holmes back on form in Birmingham\\n\\nDouble Ol...</td>\n",
       "      <td>sport</td>\n",
       "      <td>sport</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Asylum children to face returns\\n\\nThe UK gove...</td>\n",
       "      <td>politics</td>\n",
       "      <td>politics</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Tobacco giants hail court ruling\\n\\nUS tobacco...</td>\n",
       "      <td>business</td>\n",
       "      <td>business</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>Aviator 'creator' in Oscars snub\\n\\nThe man wh...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Harinordoquy suffers France axe\\n\\nNumber eigh...</td>\n",
       "      <td>sport</td>\n",
       "      <td>sport</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Solutions to net security fears\\n\\nFake bank e...</td>\n",
       "      <td>tech</td>\n",
       "      <td>tech</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Turkey-Iran mobile deal 'at risk'\\n\\nTurkey's ...</td>\n",
       "      <td>business</td>\n",
       "      <td>business</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>Police praise 'courageous' Ozzy\\n\\nRock star O...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Almagro continues Spanish surge\\n\\nUnseeded Ni...</td>\n",
       "      <td>sport</td>\n",
       "      <td>sport</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Lewis-Francis eyeing world gold\\n\\nMark Lewis-...</td>\n",
       "      <td>sport</td>\n",
       "      <td>sport</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>Poppins musical gets flying start\\n\\nThe stage...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Souness eyes summer move for Owen\\n\\nNewcastle...</td>\n",
       "      <td>sport</td>\n",
       "      <td>sport</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>Cyber criminals step up the pace\\n\\nSo-called ...</td>\n",
       "      <td>tech</td>\n",
       "      <td>tech</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Microsoft plans 'safer ID' system\\n\\nMicrosoft...</td>\n",
       "      <td>tech</td>\n",
       "      <td>tech</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Hewitt fights back to reach final\\n\\nLleyton H...</td>\n",
       "      <td>sport</td>\n",
       "      <td>sport</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               article          label  \\\n",
       "104  Holmes back on form in Birmingham\\n\\nDouble Ol...          sport   \n",
       "57   Asylum children to face returns\\n\\nThe UK gove...       politics   \n",
       "141  Tobacco giants hail court ruling\\n\\nUS tobacco...       business   \n",
       "173  Aviator 'creator' in Oscars snub\\n\\nThe man wh...  entertainment   \n",
       "2    Harinordoquy suffers France axe\\n\\nNumber eigh...          sport   \n",
       "72   Solutions to net security fears\\n\\nFake bank e...           tech   \n",
       "103  Turkey-Iran mobile deal 'at risk'\\n\\nTurkey's ...       business   \n",
       "165  Police praise 'courageous' Ozzy\\n\\nRock star O...  entertainment   \n",
       "21   Almagro continues Spanish surge\\n\\nUnseeded Ni...          sport   \n",
       "115  Lewis-Francis eyeing world gold\\n\\nMark Lewis-...          sport   \n",
       "201  Poppins musical gets flying start\\n\\nThe stage...  entertainment   \n",
       "94   Souness eyes summer move for Owen\\n\\nNewcastle...          sport   \n",
       "216  Cyber criminals step up the pace\\n\\nSo-called ...           tech   \n",
       "154  Microsoft plans 'safer ID' system\\n\\nMicrosoft...           tech   \n",
       "27   Hewitt fights back to reach final\\n\\nLleyton H...          sport   \n",
       "\n",
       "    label_pred_transfer_learning label_pred_zero_shot  \n",
       "104                        sport                 tech  \n",
       "57                      politics                 tech  \n",
       "141                     business             politics  \n",
       "173                entertainment             business  \n",
       "2                          sport             business  \n",
       "72                          tech                sport  \n",
       "103                     business             politics  \n",
       "165                entertainment             business  \n",
       "21                         sport             business  \n",
       "115                        sport             business  \n",
       "201                entertainment             business  \n",
       "94                         sport        entertainment  \n",
       "216                         tech             business  \n",
       "154                         tech             business  \n",
       "27                         sport        entertainment  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results[test_results[\"label\"]!=test_results[\"label_pred_zero_shot\"]].sample(15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There does not seem to be a particularly problematic class, although such a assertion would require further investigation. But the length of the news could lead to poor performance. We can read about this on the [ðŸ¤— Hugging Face forum](https://discuss.huggingface.co/t/new-pipeline-for-zero-shot-text-classification/681/85). Joe Davison, ðŸ¤— Hugging Face developer and creator of the Zero-Shot pipeline, says the following:\n",
    "\n",
    "> *For long documents, I donâ€™t think thereâ€™s an ideal solution right now. If truncation isnâ€™t satisfactory, then the best thing you can do is probably split the document into smaller segments and ensemble the scores somehow.*\n",
    "\n",
    "We'll try another solution: summarizing the article first, then Zero-Shot classifying it.\n",
    "\n",
    "### IV) Summarization + Zero-Shot Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to do this would have been to line up the `SummarizationPipeline` with the `ZeroShotClassificationPipeline`. This is not possible, at least with my version of the `transformers` library (3.5.1). The reason for this is that the `SummarizationPipeline` uses Facebook's BART model, whose maximal input length is 1024 tokens. However, `transformers`'s tokenizers, including `BartTokenizer`, do not automatically truncate sequences to the max input length of the corresponding model. As a consequence, the `SummarizationPipeline` crashes whenever sequences longer than 1024 tokens are given as inputs. Since there are quite a few long articles in the BBC dataset, we will have to make a custom summarization pipeline that truncates news longers than 1024 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tokenizer and model for summarization (the same that are used by default in Hugging Face's summarization pipeline)\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
    "\n",
    "model_bart = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn').to('cuda') # Run on the GPU\n",
    "tokenizer_bart = BartTokenizer.from_pretrained('facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom summarization pipeline (to handle long articles)\n",
    "def summarize(text):\n",
    "    # Tokenize and truncate\n",
    "    inputs = tokenizer_bart([text], truncation=True, max_length=1024, return_tensors='pt').to('cuda')\n",
    "    # Generate summary between 10 (by default) and 50 characters\n",
    "    summary_ids = model_bart.generate(inputs['input_ids'], num_beams=4, max_length=60, early_stopping=True)\n",
    "    # Untokenize\n",
    "    return([tokenizer_bart.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/pipelines/base.py:1012: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Apply summarization then zero-shot classification to the test set\n",
    "test_results['label_pred_sum_zs']=test_results['article'].apply(lambda x:classifier(summarize(x), candidate_labels=labels)['labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results.to_parquet(\"test_results.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Summmarization+Zero-Shot classifier pipeline: 73.54 %\n"
     ]
    }
   ],
   "source": [
    "error_rate_sum_zs=len(test_results[test_results[\"label\"]!=test_results[\"label_pred_sum_zs\"]])/len(test_results)\n",
    "print(f'Accuracy of the Summmarization+Zero-Shot classifier pipeline: {round(100*(1-error_rate_sum_zs), 2)} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the summarization before the zero-shot classification, **the accuracy jumped by ~15%**! Let us remember that there was no training whatsoever. From this perspective, a 73.5% accuracy looks pretty good. This result could probably be enhanced by tuning the summarizer's parameters regarding beam search or maximal length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V) Conclusion\n",
    "\n",
    "Text classification is a piece of cake using ðŸ¤— Hugging Face's pre-trained models: fine-tuning DistilBERT is fast (using a GPU), easy and it resulted in a 98.65% accuracy on the BBC News test set. Although this result should be confirmed with other train-test split (only 56 articles in the test set), it is absolutely remarkable. The raw Zero-Shot Classification pipeline from the `transformers` library could not compete at all with such a performance, ending up with a ~59% accuracy on the same test set. Nonetheless, this result is still decent considering the complete absence of training required by this method.  \n",
    "  \n",
    "Given the substantial length of the BBC News articles, we tried summarizing them before performing the Zero-Shot classification, still using the beloved `transformers` library. This method resulted in a +15% increase of accuracy. Another way would have been to carry out sentence segmentation before the Zero-Shot classification, and averaging the prediction over all an article's sentences.\n",
    "\n",
    "We end up with two text classifiers:\n",
    "* One that requires training and yields a 98.65% accuracy\n",
    "* One that does not require any training, but yields a ~73.5% accuracy\n",
    "\n",
    "Long live ðŸ¤— Hugging Face!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[[1]](https://arxiv.org/abs/1910.01108) Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf. *DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.* (Hugging Face, 2020)  \n",
    "  \n",
    "[[2]](http://mlg.ucd.ie/files/publications/greene06icml.pdf) D. Greene and P. Cunningham. \"Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering\", Proc. ICML 2006."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
