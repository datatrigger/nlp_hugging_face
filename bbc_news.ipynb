{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP with ðŸ¤— Hugging Face\n",
    "\n",
    "## Text Classification: Transfer Learning vs Zero-Shot Classifier\n",
    "\n",
    "ðŸ¤— [Hugging Face](https://huggingface.co/) is, in my opinion, one of the best things that has happened to Data Science over the past few years. From generalizing access to state-of-the-art NLP models with the [`transformers`](https://huggingface.co/transformers/) library to [distillation [1]](https://arxiv.org/abs/1910.01108), they are having a huge impact on the field. I recently found out about \"Zero-Shot Classification\". These models are classifiers that do not need any fine-tuning, apart from being told which classes it should predict. They are built on top of Natural Language Inference models, whose task is determining if sentence *A* implies, contradicts or has nothing to do with sentence *B*. This excellent [blog post](https://joeddav.github.io/blog/2020/05/29/ZSL.html) written by ðŸ¤— Hugging Face researcher Joe Davison provides more in-depth explanations.  \n",
    "  \n",
    "Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/bart-large-mnli were not used when initializing BartModel: ['model.encoder.version', 'model.decoder.version']\n",
      "- This IS expected if you are initializing BartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/bart-large-mnli were not used when initializing BartForSequenceClassification: ['model.encoder.version', 'model.decoder.version']\n",
      "- This IS expected if you are initializing BartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'Parallel processing with GPUs is the savior of Deep Learning',\n",
       " 'labels': ['technology', 'education', 'politics'],\n",
       " 'scores': [0.9941806197166443, 0.0031008964870125055, 0.0027184495702385902]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transformers 3.5.1 in this notebook\n",
    "from transformers import pipeline\n",
    "\n",
    "# By default, the pipeline runs on the CPU (device=-1). Set device to 0 to use the GPU (and to 1 for the second GPU, etc...)\n",
    "classifier = pipeline(\"zero-shot-classification\", device=0)\n",
    "classifier(\n",
    "    \"Parallel processing with GPUs is the savior of Deep Learning\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"technology\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier guessed that the sentence is about tech with a probability over 99%. **But how does Zero-Shot classification compare with plain \"old\" fine-tuned text classification?**\n",
    "\n",
    "### I) BBC News dataset\n",
    "\n",
    "Let's build a classifier of news articles labeled *business*, *entertainment*, *politics*, *sport* and *tech*. Available [here](http://mlg.ucd.ie/datasets/bbc.html), the dataset consists of 2225 documents from the BBC news website from the years 2004/2005. It was originally built for a Machine Learning paper about clustering [[2]](http://mlg.ucd.ie/files/publications/greene06icml.pdf).  \n",
    "  \n",
    "Articles are individual .txt files spread into 5 folders, one for each folder. The listing below puts articles/labels into a `pandas.DataFrame()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities to handle directories, files, paths, etc...\n",
    "from os import listdir\n",
    "from os.path import isdir, isfile, join\n",
    "from pathlib import Path\n",
    "\n",
    "# Most original import ever\n",
    "import pandas as pd\n",
    "\n",
    "path_to_bbc_articles=\"bbc\"\n",
    "labels=[] # labels for the text classification\n",
    "label_dataframes=[] # for each label, get the articles into a dataframe\n",
    "\n",
    "for label in [dir for dir in listdir(path_to_bbc_articles) if isdir(join(path_to_bbc_articles, dir)) and dir!=\".ipynb_checkpoints\"]:\n",
    "    labels.append(label)\n",
    "    label_path=join(path_to_bbc_articles, label)\n",
    "    articles_list=[]\n",
    "    for article_file in [file for file in listdir(label_path) if isfile(join(label_path, file))]:\n",
    "        article_path=join(label_path, article_file)\n",
    "        article=Path(article_path).read_text(encoding=\"ISO-8859-1\") # Tried utf-8 (of course) but encountered error\n",
    "        # Stackoverflow said \"try ISO-8859-1\", it worked (dataset is 11 years old)\n",
    "        articles_list.append(article)\n",
    "    label_dataframes.append(pd.DataFrame({'label': label, 'article': articles_list}))\n",
    "    \n",
    "df=pd.concat(label_dataframes, ignore_index=True) # Concatenate all the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "sport            511\n",
       "business         510\n",
       "politics         417\n",
       "tech             401\n",
       "entertainment    386\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of articles per label\n",
    "df.value_counts('label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need integer labels to feed the transformer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label_int']=df['label'].apply(lambda x:labels.index(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are 5 random rows from the final dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>article</th>\n",
       "      <th>label_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>politics</td>\n",
       "      <td>MPs tout Lords replacement plan\\n\\nA group of ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>sport</td>\n",
       "      <td>Sculthorpe wants Lions captaincy\\n\\nPaul Scult...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1644</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>Band Aid retains number one spot\\n\\nThe charit...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>politics</td>\n",
       "      <td>PM apology over jailings\\n\\nTony Blair has apo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232</th>\n",
       "      <td>business</td>\n",
       "      <td>India's Deccan gets more planes\\n\\nAir Deccan ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              label                                            article  \\\n",
       "414        politics  MPs tout Lords replacement plan\\n\\nA group of ...   \n",
       "420           sport  Sculthorpe wants Lions captaincy\\n\\nPaul Scult...   \n",
       "1644  entertainment  Band Aid retains number one spot\\n\\nThe charit...   \n",
       "416        politics  PM apology over jailings\\n\\nTony Blair has apo...   \n",
       "1232       business  India's Deccan gets more planes\\n\\nAir Deccan ...   \n",
       "\n",
       "      label_int  \n",
       "414           0  \n",
       "420           1  \n",
       "1644          3  \n",
       "416           0  \n",
       "1232          2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II) Fine-tuning a pretrained text classifier\n",
    "\n",
    "After building the train/validation/test sets, we will go straight the point by using the [`DistilBERT`](https://huggingface.co/transformers/model_doc/distilbert.html) pre-trained transformer model (and its tokenizer).\n",
    "\n",
    "> *It is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERTâ€™s performances.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set, validation set and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val, test = train_test_split(df, test_size=0.1, random_state=42, shuffle=True)\n",
    "train, val = train_test_split(train_val, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Reset the indexes of the 3 pandas.DataFrame()\n",
    "train, val, test = map(lambda x:x.reset_index(drop=True), [train, val, test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize\n",
    "\n",
    "Loading DistilBERT's tokenizer, we can see that this transformer model takes input sequences composed of up to 512 tokens: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'distilbert-base-uncased': 512,\n",
       " 'distilbert-base-uncased-distilled-squad': 512,\n",
       " 'distilbert-base-cased': 512,\n",
       " 'distilbert-base-cased-distilled-squad': 512,\n",
       " 'distilbert-base-german-cased': 512,\n",
       " 'distilbert-base-multilingual-cased': 512}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Distilbert's tokenizer\n",
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "tokenizer.max_model_input_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this compare with the lengths of the tokenized BBC articles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2225.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>487.627416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>293.625901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>108.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>314.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>423.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>592.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5303.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            length\n",
       "count  2225.000000\n",
       "mean    487.627416\n",
       "std     293.625901\n",
       "min     108.000000\n",
       "25%     314.000000\n",
       "50%     423.000000\n",
       "75%     592.000000\n",
       "max    5303.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_articles_lengths=pd.DataFrame({'length': list(map(len, tokenizer(df['article'].to_list(), truncation=False, padding=False)['input_ids']))})\n",
    "tokenized_articles_lengths.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The articles are, on average, 488-token-long. The longest news is composed of 5303 tokens. This means that an important part of the articles will be truncated before being fed to the transformer model. Here is the distribution of the lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "ax=sns.histplot(tokenized_articles_lengths)\n",
    "ax.set(xlabel='Length of tokenized articles', ylabel='Count', xlim=(0, 1200), title='Distribution of the tokenized articles lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentile of length=512: 64th\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import percentileofscore\n",
    "print(f'Percentile of length=512: {int(percentileofscore(tokenized_articles_lengths[\"length\"],512))}th')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 36% of the articles will be truncated to fit the 512-token limit of DistilBERT. The truncation is mandatory, otherwise the model crashes. We will use fixed padding for the sake of simplicity here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tune DistilBERT\n",
    "\n",
    "The train/validation/test sets must be procesÎ¼sed to work with either PyTorch or TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the train/validation/test sets\n",
    "train_encodings = tokenizer(train['article'].to_list(), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val['article'].to_list(), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test['article'].to_list(), truncation=True, padding=True)\n",
    "\n",
    "import torch\n",
    "\n",
    "class BBC_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = BBC_Dataset(train_encodings, train['label_int'].to_list())\n",
    "val_dataset = BBC_Dataset(val_encodings, val['label_int'].to_list())\n",
    "test_dataset = BBC_Dataset(test_encodings, test['label_int'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning\n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# The number of predicted labels must be specified with num_labels\n",
    "# .to('cuda') to do the training on the GPU\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(labels)).to('cuda')\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='603' max='603' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [603/603 04:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.143400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=603, training_loss=0.11982744764135055)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"bbc_news_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='56' max='56' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [56/56 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate predictions for the test set\n",
    "predictions=trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results=test.copy(deep=True)\n",
    "test_results[\"label_int_pred_transfer_learning\"]=predictions.label_ids\n",
    "test_results['label_pred_transfer_learning']=test_results['label_int_pred_transfer_learning'].apply(lambda x:labels[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>article</th>\n",
       "      <th>label_int</th>\n",
       "      <th>label_int_pred_transfer_learning</th>\n",
       "      <th>label_pred_transfer_learning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [label, article, label_int, label_int_pred_transfer_learning, label_pred_transfer_learning]\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results[test_results[\"label\"]!=test_results[\"label_pred_transfer_learning\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the fine-tuned DistilBERT transformer model on the test set is **100%**.\n",
    "\n",
    "### III) Zero-Shot Classification\n",
    "\n",
    "We'll use the appropriate [`transformers.pipeline`](https://huggingface.co/transformers/main_classes/pipelines.html) to compute the predicted class for each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/bart-large-mnli were not used when initializing BartModel: ['model.encoder.version', 'model.decoder.version']\n",
      "- This IS expected if you are initializing BartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/bart-large-mnli were not used when initializing BartForSequenceClassification: ['model.encoder.version', 'model.decoder.version']\n",
      "- This IS expected if you are initializing BartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\", device=0) # device=0 means GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the predicted label for each article\n",
    "test_results['label_pred_zero_shot']=test_results['article'].apply(lambda x:classifier(x, candidate_labels=labels)['labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns and save results\n",
    "test_results=test_results[['article', 'label', 'label_pred_transfer_learning', 'label_pred_zero_shot']]\n",
    "test_results.to_parquet(\"test_results.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results=pd.read_parquet(\"test_results.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Zero-Shot classifier: 55.16 %\n"
     ]
    }
   ],
   "source": [
    "error_rate=len(test_results[test_results[\"label\"]!=test_results[\"label_pred_zero_shot\"]])/len(test_results)\n",
    "print(f'Accuracy of the Zero-Shot classifier: {round(100*(1-error_rate), 2)} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Zero-Shot classifier does a really bad job compared with the fine-tuned model. However, given the number of labels &mdash; 5 &mdash; this result is not that catastrophic. It is well above the 20% a random classifier would achieve (assuming balanced classes). Let's have a look at a few random articles uncorrectly labeled by the Zero-Shot classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>label</th>\n",
       "      <th>label_pred_transfer_learning</th>\n",
       "      <th>label_pred_zero_shot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Blair 'damaged' by Blunkett row\\n\\nA majority ...</td>\n",
       "      <td>politics</td>\n",
       "      <td>politics</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>New rules tackle 'sham weddings'\\n\\nNew rules ...</td>\n",
       "      <td>politics</td>\n",
       "      <td>politics</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Share boost for feud-hit Reliance\\n\\nThe board...</td>\n",
       "      <td>business</td>\n",
       "      <td>business</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Controversial film tops festival\\n\\nA controve...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Gazprom 'in $36m back-tax claim'\\n\\nThe nuclea...</td>\n",
       "      <td>business</td>\n",
       "      <td>business</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>E-University 'disgraceful waste'\\n\\nA failed g...</td>\n",
       "      <td>politics</td>\n",
       "      <td>politics</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Sony PSP console hits US in March\\n\\nUS gamers...</td>\n",
       "      <td>tech</td>\n",
       "      <td>tech</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>IMF agrees fresh Turkey funding\\n\\nTurkey has ...</td>\n",
       "      <td>business</td>\n",
       "      <td>business</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Williams stays on despite dispute\\n\\nMatt Will...</td>\n",
       "      <td>sport</td>\n",
       "      <td>sport</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Animation charms Japan box office\\n\\nOscar-win...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Brando 'rejected Godfather role'\\n\\nLate film ...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Howard's unfinished business\\n\\n\"He's not fini...</td>\n",
       "      <td>politics</td>\n",
       "      <td>politics</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>US prepares for hybrid onslaught\\n\\nSales of h...</td>\n",
       "      <td>business</td>\n",
       "      <td>business</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>US bank boss hails 'genius' Smith\\n\\nUS Federa...</td>\n",
       "      <td>business</td>\n",
       "      <td>business</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>Guantanamo four free in weeks\\n\\nAll four Brit...</td>\n",
       "      <td>politics</td>\n",
       "      <td>politics</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               article          label  \\\n",
       "182  Blair 'damaged' by Blunkett row\\n\\nA majority ...       politics   \n",
       "113  New rules tackle 'sham weddings'\\n\\nNew rules ...       politics   \n",
       "150  Share boost for feud-hit Reliance\\n\\nThe board...       business   \n",
       "94   Controversial film tops festival\\n\\nA controve...  entertainment   \n",
       "90   Gazprom 'in $36m back-tax claim'\\n\\nThe nuclea...       business   \n",
       "82   E-University 'disgraceful waste'\\n\\nA failed g...       politics   \n",
       "44   Sony PSP console hits US in March\\n\\nUS gamers...           tech   \n",
       "177  IMF agrees fresh Turkey funding\\n\\nTurkey has ...       business   \n",
       "26   Williams stays on despite dispute\\n\\nMatt Will...          sport   \n",
       "5    Animation charms Japan box office\\n\\nOscar-win...  entertainment   \n",
       "37   Brando 'rejected Godfather role'\\n\\nLate film ...  entertainment   \n",
       "59   Howard's unfinished business\\n\\n\"He's not fini...       politics   \n",
       "163  US prepares for hybrid onslaught\\n\\nSales of h...       business   \n",
       "70   US bank boss hails 'genius' Smith\\n\\nUS Federa...       business   \n",
       "202  Guantanamo four free in weeks\\n\\nAll four Brit...       politics   \n",
       "\n",
       "    label_pred_transfer_learning label_pred_zero_shot  \n",
       "182                     politics                 tech  \n",
       "113                     politics             business  \n",
       "150                     business             politics  \n",
       "94                 entertainment                 tech  \n",
       "90                      business             politics  \n",
       "82                      politics             business  \n",
       "44                          tech        entertainment  \n",
       "177                     business             politics  \n",
       "26                         sport             politics  \n",
       "5                  entertainment             business  \n",
       "37                 entertainment                 tech  \n",
       "59                      politics             business  \n",
       "163                     business                sport  \n",
       "70                      business             politics  \n",
       "202                     politics                 tech  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results[test_results[\"label\"]!=test_results[\"label_pred_zero_shot\"]].sample(15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There does not seem to be a particularly problematic class, although such a assertion would require further investigation. But the length of the news could lead to poor performance. We can read about this on the [ðŸ¤— Hugging Face forum](https://discuss.huggingface.co/t/new-pipeline-for-zero-shot-text-classification/681/85). Joe Davison, ðŸ¤— Hugging Face developer and creator of the Zero-Shot pipeline, says the following:\n",
    "\n",
    "> *For long documents, I donâ€™t think thereâ€™s an ideal solution right now. If truncation isnâ€™t satisfactory, then the best thing you can do is probably split the document into smaller segments and ensemble the scores somehow.*\n",
    "\n",
    "We'll try another solution: summarizing the article first, then Zero-Shot classifying it.\n",
    "\n",
    "### IV) Summarization + Zero-Shot Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to do this would have been to line up the `SummarizationPipeline` with the `ZeroShotClassificationPipeline`. This is not possible, at least with my version of the `transformers` library (3.5.1). The reason for this is that the `SummarizationPipeline` uses Facebook's BART model, whose maximal input length is 1024 tokens. However, `transformers`'s tokenizers, including `BartTokenizer`, do not automatically truncate sequences to the max input length of the corresponding model. As a consequence, the `SummarizationPipeline` crashes whenever sequences longer than 1024 tokens are given as inputs. Since there are quite a few long articles in the BBC dataset, we will have to make a custom summarization pipeline that truncates news longers than 1024 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tokenizer and model for summarization (the same that are used by default in Hugging Face's summarization pipeline)\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
    "\n",
    "model_bart = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn').to('cuda') # Run on the GPU\n",
    "tokenizer_bart = BartTokenizer.from_pretrained('facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom summarization pipeline (to handle long articles)\n",
    "def summarize(text):\n",
    "    # Tokenize and truncate\n",
    "    inputs = tokenizer_bart([text], truncation=True, max_length=1024, return_tensors='pt').to('cuda')\n",
    "    # Generate summary between 10 (by default) and 50 characters\n",
    "    summary_ids = model_bart.generate(inputs['input_ids'], num_beams=4, max_length=50, early_stopping=True)\n",
    "    # Untokenize\n",
    "    return([tokenizer_bart.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply summarization then zero-shot classification to the test set\n",
    "test_results['label_pred_sum_zs']=test_results['article'].apply(lambda x:classifier(summarize(x), candidate_labels=labels)['labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results.to_parquet(\"test_results.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Summmarization+Zero-Shot classifier pipeline: 78.03 %\n"
     ]
    }
   ],
   "source": [
    "error_rate_sum_zs=len(test_results[test_results[\"label\"]!=test_results[\"label_pred_sum_zs\"]])/len(test_results)\n",
    "print(f'Accuracy of the Summmarization+Zero-Shot classifier pipeline: {round(100*(1-error_rate_sum_zs), 2)} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the summarization before the zero-shot classification, **the accuracy jumped by ~23%**! Let us remember that there was no training whatsoever. From this perspective, a 78% accuracy looks pretty good to me! This result could probably be enhanced by tuning the summarizer's parameters regarding beam search or maximal length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V) Conclusion\n",
    "\n",
    "Text classification is a piece of cake using ðŸ¤— Hugging Face's pre-trained models: fine-tuning DistilBERT is fast (using a GPU), easy and it resulted in a 100% accuracy on the BBC News test set. Although this result should be confirmed with other train-test split (only 56 articles in the test set), it is absolutely remarkable. The raw Zero-Shot Classification pipeline from the `transformers` library could not compete at all with such a performance, ending up with a ~55% accuracy on the same test set. Nonetheless, this result is still decent considering the complete absence of training required by this method.  \n",
    "  \n",
    "Given the substantial length of the BBC News articles, we tried summarizing them before performing the Zero-Shot classification, still using the beloved `transformers` library. This method resulted in a +23% increase of accuracy. Another way would have been to carry out sentence segmentation before the Zero-Shot classification, and averaging the prediction over all an article's sentences.\n",
    "\n",
    "We end up with two text classifiers:\n",
    "* One that requires training and yields a 100% accuracy\n",
    "* One that does not require any training, but yields a ~78% accuracy\n",
    "\n",
    "Either way, way to go ðŸ¤— Hugging Face!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[[1]](https://arxiv.org/abs/1910.01108) Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf. *DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.* (Hugging Face, 2020)  \n",
    "  \n",
    "[[2]](http://mlg.ucd.ie/files/publications/greene06icml.pdf) D. Greene and P. Cunningham. \"Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering\", Proc. ICML 2006."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
